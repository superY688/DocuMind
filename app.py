import streamlit as st
import os
import tempfile
from dotenv import load_dotenv


# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.document_processor.processor import DocumentProcessor
from src.models.model_factory import ModelFactory
from src.vector_store.vector_store import VectorStore
from src.utils.helpers import get_available_models

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="DocuMind - æ™ºèƒ½æ–‡æ¡£åˆ†æä¸é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = []

if "current_document" not in st.session_state:
    st.session_state.current_document = None

if "vector_store" not in st.session_state:
    st.session_state.vector_store = None

if "document_processor" not in st.session_state:
    st.session_state.document_processor = DocumentProcessor()

# ä¾§è¾¹æ 
with st.sidebar:
    st.title("ğŸ“š DocuMind")
    st.subheader("æ™ºèƒ½æ–‡æ¡£åˆ†æä¸é—®ç­”ç³»ç»Ÿ")
    
    # æ¨¡å‹é€‰æ‹©
    available_models = get_available_models()
    selected_model = st.selectbox(
        "é€‰æ‹©å¤§è¯­è¨€æ¨¡å‹",
        options=available_models.keys(),
        index=0,
        help="é€‰æ‹©ç”¨äºå›ç­”é—®é¢˜çš„å¤§è¯­è¨€æ¨¡å‹"
    )
    
    # æ–‡æ¡£ä¸Šä¼ 
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£", 
        type=["pdf", "docx", "txt"], 
        help="æ”¯æŒPDFã€Wordå’ŒTXTæ ¼å¼"
    )
    
    # ä¸Šä¼ æ–‡æ¡£å¤„ç†
    if uploaded_file and (st.session_state.current_document == uploaded_file.name):
        with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            temp_dir = tempfile.mkdtemp()
          
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getvalue())
            
            # å¤„ç†æ–‡æ¡£
            try:
                document_processor = st.session_state.document_processor
                document_chunks = document_processor.process_document(temp_path)
                
                # åˆ›å»ºå‘é‡å­˜å‚¨
                vector_store = VectorStore()
                vector_store.add_documents(document_chunks)
                
                # æ›´æ–°ä¼šè¯çŠ¶æ€
                st.session_state.current_document = uploaded_file.name
                st.session_state.vector_store = vector_store
                st.session_state.conversation_history = []
                
                st.success(f"æ–‡æ¡£ '{uploaded_file.name}' å·²æˆåŠŸå¤„ç†ï¼")
            except Exception as e:
                st.error(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
    
    # é«˜çº§è®¾ç½®æŠ˜å é¢æ¿
    with st.expander("é«˜çº§è®¾ç½®"):
        top_k = st.slider(
            "æ£€ç´¢æ–‡æ¡£æ•°é‡", 
            min_value=1, 
            max_value=10, 
            value=3,
            help="ä»æ–‡æ¡£ä¸­æ£€ç´¢çš„ç›¸å…³ç‰‡æ®µæ•°é‡"
        )
        
        temperature = st.slider(
            "æ¸©åº¦", 
            min_value=0.0, 
            max_value=1.0, 
            value=0.7, 
            step=0.1,
            help="æ§åˆ¶å›ç­”çš„åˆ›é€ æ€§ï¼Œè¾ƒä½çš„å€¼ä½¿å›ç­”æ›´ç¡®å®šï¼Œè¾ƒé«˜çš„å€¼ä½¿å›ç­”æ›´å¤šæ ·åŒ–"
        )
    
    # æ¸…é™¤å¯¹è¯æŒ‰é’®
    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.conversation_history = []
        st.experimental_rerun()
    
    st.divider()
    st.caption("Â© 2023 DocuMind - æ™ºèƒ½æ–‡æ¡£åˆ†æä¸é—®ç­”ç³»ç»Ÿ")

# ä¸»ç•Œé¢
st.title("æ™ºèƒ½æ–‡æ¡£åˆ†æä¸é—®ç­”ç³»ç»Ÿ")

# æ˜¾ç¤ºå½“å‰æ–‡æ¡£ä¿¡æ¯
if st.session_state.current_document:
    st.info(f"å½“å‰æ–‡æ¡£: {st.session_state.current_document}")
else:
    st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£ä»¥å¼€å§‹å¯¹è¯")

# æ˜¾ç¤ºå¯¹è¯å†å²
for i, (question, answer) in enumerate(st.session_state.conversation_history):
    with st.chat_message("user"):
        st.write(question)
    with st.chat_message("assistant"):
        st.write(answer)

# é—®é¢˜è¾“å…¥
if st.session_state.current_document:
    question = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")
    
    if question:
        # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
        with st.chat_message("user"):
            st.write(question)
        
        # ç”Ÿæˆå›ç­”
        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                try:
                    # ä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£
                    relevant_docs = st.session_state.vector_store.similarity_search(question, k=top_k)
                    
                    # åˆå§‹åŒ–æ¨¡å‹
                    model_info = available_models[selected_model]
                    model = ModelFactory.get_model(
                        model_type=model_info["type"],
                        model_name=model_info["name"],
                        temperature=temperature
                    )

                    # ç”Ÿæˆå›ç­”
                    answer_container = st.empty()
                    full_answer = ""

                    # æµå¼è¾“å‡º
                    for token in model.generate_stream(question, relevant_docs):
                        full_answer += token
                        answer_container.markdown(full_answer + "â–Œ")
                    
                    answer_container.markdown(full_answer)
                    
                    # ä¿å­˜å¯¹è¯å†å²
                    st.session_state.conversation_history.append((question, full_answer))
                    
                except Exception as e:
                    st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
else:
    st.info("è¯·å…ˆä¸Šä¼ æ–‡æ¡£ä»¥å¼€å§‹å¯¹è¯")